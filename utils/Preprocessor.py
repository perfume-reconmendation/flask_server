def Preprocessing(doc, stopwords={' '}):
    # words = tokenize(doc)
    # words = map(to_small_letter, words)
    # words = [except_non_english(pattern, w) for w in words]
    # words = map(trim, words)
    # words = [w for w in words if len(w) > 2]
    # words = remove_stopwords(words, stopwords)
    # # words = map(correction, words)
    # words = lemmatize_with_pos(words)
    # words = [w for w in words if len(w) > 2]
    # return words
    return ['I', 'be', 'boy']
